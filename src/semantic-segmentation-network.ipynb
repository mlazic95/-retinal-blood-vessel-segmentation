{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import seaborn as sns\n",
    "from pylab import imread, imshow\n",
    "from skimage.color import rgb2gray\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, core , UpSampling2D\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "#from fastai.models.unet import *\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_opt():\n",
    "    #All parameters are defualt mentioned in paper\n",
    "    return Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_conv_block_batch_norm(inputs, n_filters, dropout, kernel_size=3, batchNorm = True):\n",
    "    net = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
    "               padding=\"same\")(inputs)\n",
    "    if batchNorm:\n",
    "        net = BatchNormalization()(net)\n",
    "    net = Activation(\"relu\")(net)\n",
    "        # second layer\n",
    "    net = Dropout(dropout)(net)\n",
    "    net = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\", padding=\"same\")(net)\n",
    "    if batchNorm:\n",
    "        net = BatchNormalization()(net)\n",
    "    net = Activation(\"relu\")(net)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_block(input_tensor, n_filters, kernel_size=3, batchnorm=True):\n",
    "    # first layer\n",
    "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
    "               padding=\"same\")(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        # second layer\n",
    "        x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\")(x)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet_3(dropout, input_img):\n",
    "    inputs = input_img\n",
    "    patch_size = 32\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same',data_format='channels_last')(inputs)\n",
    "    conv1 = Dropout(dropout)(conv1)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same',data_format='channels_last')(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    #\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same',data_format='channels_last')(pool1)\n",
    "    conv2 = Dropout(dropout)(conv2)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same',data_format='channels_last')(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    #\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same',data_format='channels_last')(pool2)\n",
    "    conv3 = Dropout(dropout)(conv3)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same',data_format='channels_last')(conv3)\n",
    "\n",
    "    up1 = UpSampling2D(size=(2, 2))(conv3)\n",
    "    up1 = concatenate([conv2,up1])\n",
    "    conv4 = Conv2D(64, (3, 3), activation='relu', padding='same',data_format='channels_last')(up1)\n",
    "    conv4 = Dropout(dropout)(conv4)\n",
    "    conv4 = Conv2D(64, (3, 3), activation='relu', padding='same',data_format='channels_last')(conv4)\n",
    "    #\n",
    "    up2 = UpSampling2D(size=(2, 2))(conv4)\n",
    "    up2 = concatenate([conv1,up2])\n",
    "    conv5 = Conv2D(32, (3, 3), activation='relu', padding='same',data_format='channels_last')(up2)\n",
    "    conv5 = Dropout(dropout)(conv5)\n",
    "    conv5 = Conv2D(32, (3, 3), activation='relu', padding='same',data_format='channels_last')(conv5)\n",
    "    #\n",
    "    conv6 = Conv2D(2, (1, 1), activation='relu',padding='same',data_format='channels_last')(conv5)\n",
    "    conv6 = core.Reshape((2,32*32))(conv6)\n",
    "    conv6 = core.Permute((2,1))(conv6)\n",
    "    ############\n",
    "    conv7 = core.Activation('softmax')(conv6)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=conv7)\n",
    "    opt  = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    model.compile(optimizer = opt, loss = 'binary_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_gen(dropout, input_img, transposedConv, n_filters = 32):\n",
    "    inputs = input_img\n",
    "    patch_size = 32\n",
    "    block1 = encoder_conv_block_batch_norm(inputs, n_filters =n_filters,\n",
    "                                  kernel_size=3, dropout = dropout*0.5)\n",
    "    pool1 = MaxPooling2D((2, 2))(block1)\n",
    "    #conv1 = Conv2D(32, (3, 3), activation='relu', padding='same',data_format='channels_last')(inputs)\n",
    "    #conv1 = Dropout(dropout)(conv1)\n",
    "    #conv1 = Conv2D(32, (3, 3), activation='relu', padding='same',data_format='channels_last')(conv1)\n",
    "    #pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    #\n",
    "    block2 = encoder_conv_block_batch_norm(pool1, n_filters = n_filters*2,\n",
    "                                  kernel_size=3, dropout = dropout)\n",
    "    pool2 = MaxPooling2D((2, 2))(block2) \n",
    "    #conv2 = Conv2D(64, (3, 3), activation='relu', padding='same',data_format='channels_last')(pool1)\n",
    "    #conv2 = Dropout(dropout)(conv2)\n",
    "    #conv2 = Conv2D(64, (3, 3), activation='relu', padding='same',data_format='channels_last')(conv2)\n",
    "    #pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    #\n",
    "    bridge = encoder_conv_block_batch_norm(pool2, n_filters = n_filters*4,\n",
    "                                  kernel_size=3, dropout = dropout)\n",
    "    \n",
    "    #conv3 = Conv2D(128, (3, 3), activation='relu', padding='same',data_format='channels_last')(pool2)\n",
    "    #conv3 = Dropout(dropout)(conv3)\n",
    "    #conv3 = Conv2D(128, (3, 3), activation='relu', padding='same',data_format='channels_last')(conv3)\n",
    "    up1 = bridge\n",
    "    if transposedConv:\n",
    "        up1 = Conv2DTranspose(n_filters*4, (3, 3), strides=(2, 2), padding='same') (up1)\n",
    "    else:\n",
    "        up1 = UpSampling2D(size=(2, 2))(up1)\n",
    "    up1 = concatenate([block2,up1])\n",
    "    up_block2 = encoder_conv_block_batch_norm(up1, n_filters = n_filters*2,\n",
    "                                  kernel_size=3, dropout= dropout)\n",
    "    \n",
    "    #conv4 = Conv2D(64, (3, 3), activation='relu', padding='same',data_format='channels_last')(up1)\n",
    "    #conv4 = Dropout(dropout)(conv4)\n",
    "    #conv4 = Conv2D(64, (3, 3), activation='relu', padding='same',data_format='channels_last')(conv4)\n",
    "    #\n",
    "    up2 = up_block2\n",
    "    if transposedConv:\n",
    "        up2 = Conv2DTranspose(n_filters*2, (3, 3), strides=(2, 2), padding='same') (up2)\n",
    "    else:\n",
    "        up2 = UpSampling2D(size=(2, 2))(up2)\n",
    "    up2 = concatenate([block1,up2])\n",
    "    up_block1 = encoder_conv_block_batch_norm(up2, n_filters = n_filters,\n",
    "                                  kernel_size=3, dropout = dropout)\n",
    "    #conv5 = Conv2D(32, (3, 3), activation='relu', padding='same',data_format='channels_last')(up2)\n",
    "    #conv5 = Dropout(dropout)(conv5)\n",
    "    #conv5 = Conv2D(32, (3, 3), activation='relu', padding='same',data_format='channels_last')(conv5)\n",
    "    #\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid',padding='same')(up_block1)\n",
    "    #conv6 = core.Reshape((2,32*32))(conv6)\n",
    "    #conv6 = core.Permute((2,1))(conv6)\n",
    "    ############\n",
    "    #conv7 = core.Activation('softmax')(conv6)\n",
    "    opt = get_opt()\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer = opt, loss = 'binary_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transp_conv_unet(dropout, input_img):\n",
    "    return unet_gen(dropout = dropout, input_img = input_img, transposedConv = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_up_sample_unet(dropout, input_img):\n",
    "    return unet_gen(dropout = dropout, input_img = input_img, transposedConv = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet_seismic(input_img, dropout, n_filters=16, batchnorm=True):\n",
    "    # contracting path\n",
    "    print(\"SEISMIC UNET\")\n",
    "    c1 = conv2d_block(input_img, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
    "    p1 = MaxPooling2D((2, 2)) (c1)\n",
    "    p1 = Dropout(dropout)(p1)\n",
    "    c2 = conv2d_block(p1, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
    "    p2 = MaxPooling2D((2, 2)) (c2)\n",
    "    p2 = Dropout(dropout)(p2)\n",
    "    c3 = conv2d_block(p2, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
    "    p3 = MaxPooling2D((2, 2)) (c3)\n",
    "    p3 = Dropout(dropout)(p3)\n",
    "    c4 = conv2d_block(p3, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "    p4 = Dropout(dropout)(p4)\n",
    "    c5 = conv2d_block(p4, n_filters=n_filters*16, kernel_size=3, batchnorm=batchnorm)\n",
    "    # expansive path\n",
    "    u6 = Conv2DTranspose(n_filters*8, (3, 3), strides=(2, 2), padding='same') (c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    u6 = Dropout(dropout)(u6)\n",
    "    c6 = conv2d_block(u6, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
    "    u7 = Conv2DTranspose(n_filters*4, (3, 3), strides=(2, 2), padding='same') (c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    u7 = Dropout(dropout)(u7)\n",
    "    c7 = conv2d_block(u7, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
    "    u8 = Conv2DTranspose(n_filters*2, (3, 3), strides=(2, 2), padding='same') (c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    u8 = Dropout(dropout)(u8)\n",
    "    c8 = conv2d_block(u8, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
    "    u9 = Conv2DTranspose(n_filters*1, (3, 3), strides=(2, 2), padding='same') (c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    u9 = Dropout(dropout)(u9)\n",
    "    c9 = conv2d_block(u9, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    opt  = get_opt()\n",
    "    model.compile(optimizer = opt, loss = 'binary_crossentropy')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder_path):\n",
    "    file_names = os.listdir(folder_path)\n",
    "    file_names.sort()\n",
    "    images = []\n",
    "    for filename in file_names:\n",
    "        try:\n",
    "            image =imread(folder_path + filename)\n",
    "            images.append(rgb2gray(image).reshape(32,32,1))\n",
    "        except:\n",
    "            continue\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(x_train, y_train, batch_size):\n",
    "    data_gen = ImageDataGenerator(\n",
    "        horizontal_flip = True, vertical_flip = True,\n",
    "    ).flow(x_train, x_train, batch_size, seed=1)\n",
    "    mask_gen = ImageDataGenerator(\n",
    "        horizontal_flip = True, vertical_flip = True\n",
    "    ).flow(y_train, y_train, batch_size, seed=1)\n",
    "    while True:\n",
    "        x_batch, _ = data_gen.next()\n",
    "        y_batch, _ = mask_gen.next()\n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data augmentation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = np.array(load_images_from_folder(\"../../data/DRIVE/training/patches/\"))\n",
    "y_train = np.array(load_images_from_folder(\"../../data/DRIVE/training/patchLabels/\"))\n",
    "image_batch, mask_batch = next(generator(X_train, y_train, 8))\n",
    "fix, ax = plt.subplots(8,2, figsize=(8,20))\n",
    "for i in range(8):\n",
    "    ax[i,0].imshow(image_batch[i,:,:,0])\n",
    "    ax[i,1].imshow(mask_batch[i,:,:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, n_folds, model_dict):\n",
    "    X = np.array(load_images_from_folder(\"../../data/DRIVE/training/patches/\"))\n",
    "    y = np.array(load_images_from_folder(\"../../data/DRIVE/training/patchLabels/\"))\n",
    "    load_model = model_dict['load_model']\n",
    "    args = model_dict['args']\n",
    "    model = load_model(**args)\n",
    "    batch_size = 32\n",
    "    historyList =[]\n",
    "    if n_folds > 0:\n",
    "        if (len(X) % n_folds) != 0:\n",
    "            raise Exception('Same eye in validation and traning set, Patches: ', X_train.shape[1], ' Folds', n_folds)\n",
    "        sizeFold = len(X) // n_folds\n",
    "        fold_idx = []\n",
    "        for fold in range(n_folds):\n",
    "            for curIdx in range(sizeFold):\n",
    "                fold_idx.append(fold)\n",
    "        fold_idx = np.array(fold_idx)\n",
    "    \n",
    "        for fold in range(n_folds):\n",
    "            train_idx = fold_idx != fold\n",
    "            test_idx = fold_idx == fold\n",
    "            X_train = X[train_idx]\n",
    "            y_train = y[train_idx]\n",
    "            X_test = X[test_idx]\n",
    "            y_test = y[test_idx]\n",
    "            history = model.fit_generator(generator(X_train, y_train, batch_size = batch_size),\n",
    "                                  epochs = epochs, steps_per_epoch = len(X_train)//batch_size,\n",
    "                                         validation_data = (X_test,y_test))\n",
    "            historyList.append(history)\n",
    "    else : \n",
    "        print(X.shape)\n",
    "        print(y.shape)\n",
    "        \n",
    "        history = model.fit_generator(generator(X, y, batch_size = batch_size),\n",
    "                                      epochs = epochs, steps_per_epoch = len(X)//batch_size)\n",
    "        historyList.append(history)\n",
    "    return model, historyList\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on the full data set, no cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 32, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The input must have 3 channels; got `input_shape=(32, 32, 1)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-330-db2e1220412d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0margs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'input_img'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'load_model'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtransfer_learning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'args'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0margs1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistoryList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-166-d55f1f4bd2e7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, n_folds, model_dict)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mload_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'load_model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'args'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mhistoryList\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-319-8e44d2c9cfae>\u001b[0m in \u001b[0;36mtransfer_learning\u001b[0;34m(input_img)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVGG16\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mPTModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mbase_pretrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPTModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mbase_pretrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# base_pretrained_model.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programmering/DLProject/retinal-blood-vessel-segmentation/RBVS/lib/python3.6/site-packages/keras/applications/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'utils'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbase_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programmering/DLProject/retinal-blood-vessel-segmentation/RBVS/lib/python3.6/site-packages/keras/applications/vgg16.py\u001b[0m in \u001b[0;36mVGG16\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mkeras_modules_injection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mvgg16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programmering/DLProject/retinal-blood-vessel-segmentation/RBVS/lib/python3.6/site-packages/keras_applications/vgg16.py\u001b[0m in \u001b[0;36mVGG16\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m                                       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                                       \u001b[0mrequire_flatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                                       weights=weights)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programmering/DLProject/retinal-blood-vessel-segmentation/RBVS/lib/python3.6/site-packages/keras_applications/imagenet_utils.py\u001b[0m in \u001b[0;36m_obtain_input_shape\u001b[0;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                     raise ValueError('The input must have 3 channels; got '\n\u001b[0;32m--> 316\u001b[0;31m                                      '`input_shape=' + str(input_shape) + '`')\n\u001b[0m\u001b[1;32m    317\u001b[0m                 if ((input_shape[0] is not None and input_shape[0] < min_size) or\n\u001b[1;32m    318\u001b[0m                    (input_shape[1] is not None and input_shape[1] < min_size)):\n",
      "\u001b[0;31mValueError\u001b[0m: The input must have 3 channels; got `input_shape=(32, 32, 1)`"
     ]
    }
   ],
   "source": [
    "img = patches[0].reshape(1,32,32,1)\n",
    "print(img.shape)\n",
    "args1 = {'input_img' : img}\n",
    "model_dict = {'load_model' : transfer_learning, 'args' : args1}\n",
    "res, historyList = train(epochs = 1, n_folds = 0, model_dict = model_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare n models with different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "args1 = {'input_img' : Input((32,32,1)), 'dropout' : 0.05}\n",
    "args2 = {'input_img' : Input((32,32,1)), 'dropout' : 0.2}\n",
    "args3 = {'input_img' : Input((32,32,1)), 'dropout' : 0.3}\n",
    "\n",
    "model_dict1 = {'load_model' : get_transp_conv_unet, 'args' : args1, 'name': 'TranspConv0.05'}\n",
    "model_dict2 = {'load_model' : get_unet_seismic, 'args' : args1, 'name': 'Seismic'}\n",
    "model_dict3 = {'load_model' : get_up_sample_unet, 'args' : args1, 'name': 'UpSample'}\n",
    "model_dict4 = {'load_model' : get_transp_conv_unet, 'args' : args2, 'name': 'TranspConv0.2'}\n",
    "model_dict5 = {'load_model' : get_transp_conv_unet, 'args' : args3, 'name': 'TranspConv0.3'}\n",
    "model_dict6 = {'load_model' : get_unet_seismic, 'args' : args2, 'name': 'Seismic0.2'}\n",
    "\n",
    "\n",
    "\n",
    "model_dicts = [model_dict1, model_dict2, model_dict3]\n",
    "models = []\n",
    "historyLists =[]\n",
    "for model_dict in model_dicts:\n",
    "    res, historyList = train(epochs = epochs, n_folds = 5, model_dict = model_dict)\n",
    "    models.append(res)\n",
    "    historyLists.append(historyList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_history_df = pd.DataFrame()\n",
    "for cur_model_nr in range(len(models)):\n",
    "    loss_data = ([i.history['loss'] for i in historyLists[cur_model_nr]])\n",
    "    val_loss_data = ([i.history['val_loss'] for i in historyLists[cur_model_nr]])\n",
    "    all_data = loss_data + val_loss_data\n",
    "    all_data.append( [i+1 for i in range(epochs)])\n",
    "    model_history_df = pd.DataFrame(all_data).T\n",
    "    column_names = []\n",
    "    foldLossName = []\n",
    "    for i in range(len(loss_data)):\n",
    "        foldLossName.append(\"fold\"+str(i)+\"_loss\")\n",
    "    column_names += foldLossName\n",
    "    foldValLossName = []\n",
    "    for i in range(len(val_loss_data)):\n",
    "        foldValLossName.append(\"fold\"+str(i)+\"val_loss\")\n",
    "    column_names += foldValLossName\n",
    "    column_names.append('epochs')\n",
    "    model_history_df.columns=column_names\n",
    "    model_history_df[\"model\"] = model_dicts[cur_model_nr]['name']\n",
    "    #plt.plot(history.history['loss'])\n",
    "    #plt.plot(history.history['val_loss'])\n",
    "    #plt.xlabel(\"epoch\")\n",
    "    #plt.ylabel(\"loss\")\n",
    "    #plt.show()\n",
    "    model_history_df = pd.melt(model_history_df, value_vars=foldLossName+ foldValLossName,\n",
    "                               id_vars = ['epochs','model'] ,value_name = \"loss\")\n",
    "    types = ['train', 'validation']\n",
    "    trainOrValidation = []\n",
    "    for i in types:\n",
    "        for j in range(model_history_df.shape[0]//2):\n",
    "            trainOrValidation.append(i)\n",
    "    model_history_df['type'] =trainOrValidation\n",
    "    model_history_df = model_history_df.astype({'epochs': int})\n",
    "    full_model_history_df = pd.concat([full_model_history_df, model_history_df], ignore_index = True)\n",
    "print(full_model_history_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "ax = sns.lineplot(x=\"epochs\", y=\"loss\", style=\"type\", hue= 'model', data=full_model_history_df)\n",
    "legend = ax.legend()\n",
    "legend.texts[0].set_text(\"\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nr_of_images = 10\n",
    "fig, ax = plt.subplots(nr_of_images,4, figsize=(8,20))\n",
    "fig.tight_layout()\n",
    "for image_nr in range(nr_of_images):\n",
    "    ax[image_nr,0].imshow(X_train[image_nr].reshape(32,32))\n",
    "    ax[image_nr,1].imshow(y_train[image_nr].reshape(32,32))\n",
    "    y_pred = res.predict(X_train[image_nr:(image_nr+1)]).reshape(32,32)\n",
    "    y_pred_thr = y_pred.copy()\n",
    "    y_pred_thr[y_pred > 0.5] = 1\n",
    "    y_pred_thr[y_pred <= 0.5] = 0\n",
    "    ax[image_nr,2].imshow(y_pred)\n",
    "    ax[image_nr,3].imshow(y_pred_thr)\n",
    "    for j in range(4):\n",
    "        ax[image_nr,j].set_xticklabels([])\n",
    "        ax[image_nr,j].set_yticklabels([])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_image(image, mask, model, m, dataset):\n",
    "    image = rgb2gray(image)\n",
    "    patches, indexes = helpers.image_to_non_overlapping_patches(image, mask, m, helpers.Dataset.DRIVE)\n",
    "    patches = np.array(patches)\n",
    "    patches = patches.reshape(patches.shape[0], m, m, 1)\n",
    "    y_pred = model.predict(patches).reshape(patches.shape[0], 32,32)\n",
    "    y_pred_thr = y_pred.copy()\n",
    "    y_pred_thr[y_pred > 0.5] = 1\n",
    "    y_pred_thr[y_pred <= 0.5] = 0\n",
    "    image = helpers.patches_to_image(y_pred_thr, indexes, m, image.shape[0], image.shape[1] )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_images = helpers.load_images_from_folder(helpers.DRIVE_TEST_IMAGES_PATH)\n",
    "drive_images_color = load_images_from_folder_color(helpers.DRIVE_TEST_IMAGES_PATH)\n",
    "drive_mask = helpers.load_images_from_folder(helpers.DRIVE_TEST_MASK_PATH)\n",
    "drive_segmented = helpers.load_images_from_folder(helpers.DRIVE_TEST_SEG_1_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_segmentation = segment_image(drive_images[0], drive_mask[0],transfer_model , 32, helpers.Dataset.DRIVE)\n",
    "fig, ax = plt.subplots(1,3, figsize=(20,20))\n",
    "ax[0].imshow(image_segmentation)\n",
    "ax[1].imshow(drive_segmented[0])\n",
    "ax[2].imshow(helpers.histogram_equalization(drive_images[0]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_learning(input_img):\n",
    "    import ssl\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "   \n",
    "    from keras.applications.vgg16 import VGG16 as PTModel\n",
    "    base_pretrained_model = PTModel(input_shape = input_img.shape[1:], include_top = False, weights = 'imagenet')\n",
    "    base_pretrained_model.trainable = False\n",
    "    # base_pretrained_model.summary()\n",
    "\n",
    "    from collections import defaultdict, OrderedDict\n",
    "    from keras.models import Model\n",
    "    layer_size_dict = defaultdict(list)\n",
    "    inputs = []\n",
    "    for lay_idx, c_layer in enumerate(base_pretrained_model.layers):\n",
    "        if not c_layer.__class__.__name__ == 'InputLayer':\n",
    "            layer_size_dict[c_layer.get_output_shape_at(0)[1:3]] += [c_layer]\n",
    "        else:\n",
    "            inputs += [c_layer]\n",
    "    # freeze dict\n",
    "    layer_size_dict = OrderedDict(layer_size_dict.items())\n",
    "    for k,v in layer_size_dict.items():\n",
    "        print(k, [w.__class__.__name__ for w in v])\n",
    "    \n",
    "    pretrained_encoder = Model(inputs = base_pretrained_model.get_input_at(0), \n",
    "                           outputs = [v[-1].get_output_at(0) for k, v in layer_size_dict.items()])\n",
    "    pretrained_encoder.trainable = False\n",
    "    n_outputs = pretrained_encoder.predict([input_img])\n",
    "    for c_out, (k, v) in zip(n_outputs, layer_size_dict.items()):\n",
    "        print(c_out.shape, 'expected', k)\n",
    "    \n",
    "    \n",
    "    from keras.layers import Input, Conv2D, concatenate, UpSampling2D, BatchNormalization, Activation, Cropping2D, ZeroPadding2D\n",
    "    x_wid, y_wid = input_img.shape[1:3]\n",
    "    in_t0 = Input(input_img.shape[1:], name = 'T0_Image')\n",
    "    wrap_encoder = lambda i_layer: {k: v for k, v in zip(layer_size_dict.keys(), pretrained_encoder(i_layer))}\n",
    "\n",
    "    t0_outputs = wrap_encoder(in_t0)\n",
    "    lay_dims = sorted(t0_outputs.keys(), key = lambda x: x[0])\n",
    "    skip_layers = 2\n",
    "    last_layer = None\n",
    "    for k in lay_dims[skip_layers:]:\n",
    "        cur_layer = t0_outputs[k]\n",
    "        channel_count = cur_layer._keras_shape[-1]\n",
    "        cur_layer = Conv2D(channel_count//2, kernel_size=(3,3), padding = 'same', activation = 'linear')(cur_layer)\n",
    "        cur_layer = BatchNormalization()(cur_layer) # gotta keep an eye on that internal covariant shift\n",
    "        cur_layer = Activation('relu')(cur_layer)\n",
    "\n",
    "        if last_layer is None:\n",
    "            x = cur_layer\n",
    "        #else:\n",
    "         #   last_channel_count = last_layer._keras_shape[-1]\n",
    "         #   x = Conv2D(last_channel_count//2, kernel_size=(3,3), padding = 'same')(last_layer)\n",
    "         #   x = UpSampling2D((2, 2))(x)\n",
    "         #   x = concatenate([cur_layer, x])\n",
    "        last_layer = x\n",
    "    final_output = Conv2D(input_img.shape[-1], kernel_size=(1,1), padding = 'same', activation = 'sigmoid')(last_layer)\n",
    "    crop_size = 20\n",
    "    final_output = Cropping2D((crop_size, crop_size))(final_output)\n",
    "    final_output = ZeroPadding2D((crop_size, crop_size))(final_output)\n",
    "    unet_model = Model(inputs = [in_t0],\n",
    "                      outputs = [final_output])\n",
    "    unet_model.summary()\n",
    "    \n",
    "    unet_model.compile(optimizer=Adam(1e-3, decay = 1e-6), \n",
    "                       loss=dice_p_bce, \n",
    "                       metrics=[dice_coef, 'binary_accuracy', true_positive_rate])\n",
    "    \n",
    "    return unet_model\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "        intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "        union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "        return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.resize(drive_images[0], 32*32*3)\n",
    "transfer_learning(img.reshape(1,32,32,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder_color(folder_path):\n",
    "    file_names = os.listdir(folder_path)\n",
    "    file_names.sort()\n",
    "    images = []\n",
    "    for filename in file_names:\n",
    "        try:\n",
    "            image =imread(folder_path + filename)\n",
    "            images.append(image)\n",
    "        except:\n",
    "            continue\n",
    "    print(images)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_p_bce(in_gt, in_pred):\n",
    "        return 0.0*binary_crossentropy(in_gt, in_pred) - dice_coef(in_gt, in_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positive_rate(y_true, y_pred):\n",
    "        return K.sum(K.flatten(y_true)*K.flatten(K.round(y_pred)))/K.sum(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches, labels = helpers.get_image_pathes(drive_images[0], drive_segmented[0], 32, 100, helpers.Dataset.DRIVE, mask=drive_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "banankernel",
   "language": "python",
   "name": "banankernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
